{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6Q0d4l0/GFn2jHWH5K2N7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/110805/Spelling_Correction/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CppjJIYuqKO",
        "colab_type": "code",
        "outputId": "a816c5ce-c378-4792-b9fb-89c9570f71eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/110805/Spelling_Correction.git\n",
        "%cd Spelling_Correction/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Spelling_Correction'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 23 (delta 9), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (23/23), done.\n",
            "/content/Spelling_Correction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn-1wK13vV65",
        "colab_type": "code",
        "outputId": "ebd02643-c8a0-4000-cae9-cd887e4eb644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "from os import system\n",
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
        "from dataloader import sample_pair\n",
        "from dataloader import Lang\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"========================================================================================\n",
        "The sample.py includes the following template functions:\n",
        "\n",
        "1. Encoder, decoder\n",
        "2. Training function\n",
        "3. BLEU-4 score function\n",
        "\n",
        "You have to modify them to complete the lab.\n",
        "In addition, there are still other functions that you have to \n",
        "implement by yourself.\n",
        "\n",
        "1. Your own dataloader (design in your own way, not necessary Pytorch Dataloader)\n",
        "2. Output your results (BLEU-4 score, correction words)\n",
        "3. Plot loss/score\n",
        "4. Load/save weights\n",
        "========================================================================================\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "#----------Hyper Parameters----------#\n",
        "hidden_size = 256\n",
        "vocab_size = 17703 #The number of words in vocabulary\n",
        "teacher_forcing_ratio = 0.7\n",
        "LR = 0.05\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "################################\n",
        "#Example inputs of compute_bleu\n",
        "################################\n",
        "#The target word\n",
        "reference = 'variable'\n",
        "#The word generated by your model\n",
        "output = 'varable'\n",
        "\n",
        "#compute BLEU-4 score\n",
        "def compute_bleu(output, reference):\n",
        "    cc = SmoothingFunction()\n",
        "    if len(reference) == 3:\n",
        "        weights = (0.33,0.33,0.33)\n",
        "    else:\n",
        "        weights = (0.25,0.25,0.25,0.25)\n",
        "    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)\n",
        "\n",
        "#Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, 1, self.hidden_size, device=device))\n",
        "\n",
        "#Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1) # crossentropyloss have done logsoftmax\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.out(output[0])\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    #----------sequence to sequence part for encoder----------#\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        #encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\t\n",
        "    #----------sequence to sequence part for decoder----------#\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    lang = Lang('train.json')\n",
        "    index = lang.addWord()\n",
        "    training_pairs = [sample_pair('train.json', lang, index) for i in range(n_iters)]\n",
        "    print('Finish sampling')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "            \n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.plot(range(int(n_iters/plot_every)), plot_losses)\n",
        "    plt.xlabel('Iterations*100')\n",
        "    plt.ylabel('CrossEntropyLoss')\n",
        "    plt.savefig('TrainingLoss')\n",
        "\n",
        "encoder1 = EncoderRNN(vocab_size, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, vocab_size).to(device)\n",
        "trainIters(encoder1, decoder1, 150000, print_every=5000)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish sampling\n",
            "0m 33s (- 16m 14s) (5000 3%) 4.9306\n",
            "1m 2s (- 14m 39s) (10000 6%) 4.8079\n",
            "1m 33s (- 14m 5s) (15000 10%) 4.7373\n",
            "2m 5s (- 13m 35s) (20000 13%) 4.6737\n",
            "2m 36s (- 13m 4s) (25000 16%) 4.6433\n",
            "3m 8s (- 12m 32s) (30000 20%) 4.5833\n",
            "3m 39s (- 12m 1s) (35000 23%) 4.5224\n",
            "4m 10s (- 11m 29s) (40000 26%) 4.4630\n",
            "4m 42s (- 10m 58s) (45000 30%) 4.3822\n",
            "5m 13s (- 10m 27s) (50000 33%) 4.2854\n",
            "5m 45s (- 9m 56s) (55000 36%) 4.1464\n",
            "6m 16s (- 9m 24s) (60000 40%) 3.9974\n",
            "6m 47s (- 8m 52s) (65000 43%) 3.8029\n",
            "7m 19s (- 8m 21s) (70000 46%) 3.5347\n",
            "7m 50s (- 7m 50s) (75000 50%) 3.2363\n",
            "8m 22s (- 7m 19s) (80000 53%) 2.9126\n",
            "8m 53s (- 6m 48s) (85000 56%) 2.5446\n",
            "9m 25s (- 6m 16s) (90000 60%) 2.2205\n",
            "9m 56s (- 5m 45s) (95000 63%) 1.8370\n",
            "10m 28s (- 5m 14s) (100000 66%) 1.5089\n",
            "10m 59s (- 4m 42s) (105000 70%) 1.2277\n",
            "11m 31s (- 4m 11s) (110000 73%) 0.9720\n",
            "12m 2s (- 3m 40s) (115000 76%) 0.7707\n",
            "12m 34s (- 3m 8s) (120000 80%) 0.5832\n",
            "13m 5s (- 2m 37s) (125000 83%) 0.4566\n",
            "13m 37s (- 2m 5s) (130000 86%) 0.3511\n",
            "14m 8s (- 1m 34s) (135000 90%) 0.2742\n",
            "14m 39s (- 1m 2s) (140000 93%) 0.2136\n",
            "15m 9s (- 0m 31s) (145000 96%) 0.1728\n",
            "15m 40s (- 0m 0s) (150000 100%) 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a8OkV1Xhz7O",
        "colab_type": "code",
        "outputId": "606fb9f9-cb41-4a8c-da80-8d49097d4c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def evaluate(encoder, decoder, lang, index, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        pairs = [sample_pair('train.json', lang, index)]\n",
        "        input_tensor = pairs[0][0]\n",
        "        target_tensor = pairs[0][1]\n",
        "        index2word = {v : k for k, v in index.items()}\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            #decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, index2word[target_tensor[0].item()]\n",
        "\n",
        "lang = Lang('train.json')\n",
        "index = lang.addWord()\n",
        "score = 0\n",
        "for i in range(50):\n",
        "    output, reference = evaluate(encoder1, decoder1, lang, index)\n",
        "    print(output)\n",
        "    print(reference)\n",
        "    if len(output) != 0:\n",
        "        score += compute_bleu(output[0], reference)\n",
        "    else:\n",
        "        score += compute_bleu('', reference)\n",
        "\n",
        "print(score/50)  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['terminal']\n",
            "terminal\n",
            "['feasible']\n",
            "feasible\n",
            "['successive']\n",
            "successive\n",
            "['united']\n",
            "united\n",
            "['ophthalmology']\n",
            "ophthalmology\n",
            "['linen']\n",
            "linen\n",
            "['primitively']\n",
            "primitively\n",
            "['disappear']\n",
            "disappear\n",
            "['long']\n",
            "long\n",
            "['practise']\n",
            "practise\n",
            "['mice']\n",
            "mice\n",
            "['announcement']\n",
            "announcement\n",
            "['scribbling']\n",
            "scribbling\n",
            "['during']\n",
            "during\n",
            "['counselors']\n",
            "counsellors\n",
            "['details']\n",
            "details\n",
            "['subtracted']\n",
            "subtracted\n",
            "['interests']\n",
            "interests\n",
            "['exact']\n",
            "exact\n",
            "['bourgeoisie']\n",
            "bourgeoisie\n",
            "['proportion']\n",
            "proportion\n",
            "['buffalos']\n",
            "buffalos\n",
            "['suffering']\n",
            "suffering\n",
            "['cent']\n",
            "cent\n",
            "['camera']\n",
            "camera\n",
            "['notice']\n",
            "notice\n",
            "['scatter']\n",
            "scatter\n",
            "['fixed']\n",
            "fixed\n",
            "['agreed']\n",
            "agreed\n",
            "['disastrous']\n",
            "disastrous\n",
            "['criticized']\n",
            "criticized\n",
            "['appropriate']\n",
            "appropriate\n",
            "['dream']\n",
            "dram\n",
            "['jar']\n",
            "jar\n",
            "['bright']\n",
            "bright\n",
            "['particularly']\n",
            "particularly\n",
            "['consistent']\n",
            "consistent\n",
            "['dividend']\n",
            "dividend\n",
            "['things']\n",
            "things\n",
            "['villain']\n",
            "villain\n",
            "['secondary']\n",
            "secondary\n",
            "['pressure']\n",
            "pressure\n",
            "['mysterious']\n",
            "mysterious\n",
            "['cupboard']\n",
            "cupboard\n",
            "['gloomy']\n",
            "gloomy\n",
            "['positron']\n",
            "positron\n",
            "['cantaloupe']\n",
            "cantaloupe\n",
            "['interrupted']\n",
            "interrupted\n",
            "['corresponds']\n",
            "corresponds\n",
            "['practitioner']\n",
            "practitioner\n",
            "0.97930425117504\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}